{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-10T07:09:53.602267Z",
     "start_time": "2026-02-10T07:09:53.594497Z"
    }
   },
   "cell_type": "markdown",
   "source": "## Let's start with basic derivative functions",
   "id": "1bba47be89913486"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:01:28.468279Z",
     "start_time": "2026-02-10T08:01:28.458826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def dy_dx(x):\n",
    "  return 2*x"
   ],
   "id": "881d52e820b1c64f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:01:28.836610Z",
     "start_time": "2026-02-10T08:01:28.829347Z"
    }
   },
   "cell_type": "code",
   "source": "dy_dx(3)\n",
   "id": "c9debeb1610732d5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Let's use pytorch lib to use derivatives",
   "id": "5b880ba30f3a8f36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:01:31.452825Z",
     "start_time": "2026-02-10T08:01:29.868337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# require_grad=True means, it will track the variable x for coming derivatives function. Note: it's not derivative the value x itself.\n",
    "x = torch.tensor(3.0, requires_grad=True)"
   ],
   "id": "8b524cc76b4fdd70",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:01:32.840312Z",
     "start_time": "2026-02-10T08:01:32.837212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = x ** 2\n",
    "\n",
    "# Note: If take derivative of x**2, then y will 2x."
   ],
   "id": "afad85c04e7628fa",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:01:33.183734Z",
     "start_time": "2026-02-10T08:01:33.177886Z"
    }
   },
   "cell_type": "code",
   "source": "x",
   "id": "62c0275447b913c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3., requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:01:33.577212Z",
     "start_time": "2026-02-10T08:01:33.571939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# In backend, calculation is happening like this: forward direction: x -> func(square(x)) -> y. This is the forward direction. We are using require_grad = true to remember the values.\n",
    "y"
   ],
   "id": "d48232c23a89f536",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:01:34.154845Z",
     "start_time": "2026-02-10T08:01:34.121935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# In backend, calculation is happening like this: backward direction: x <- dy/dx of func(square(x)) <- y. This is the backward direction. Final value of x after backward direction will 6\n",
    "y.backward()"
   ],
   "id": "3ef36f2904213335",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:01:41.455314Z",
     "start_time": "2026-02-10T08:01:41.450114Z"
    }
   },
   "cell_type": "code",
   "source": "y",
   "id": "e3e24a632a2bd06f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:01:47.983263Z",
     "start_time": "2026-02-10T08:01:47.977731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# result of derivative w.r.t x is store using grad\n",
    "x.grad"
   ],
   "id": "e7d5a610e38098e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Let's use another example: dz/dx = dy/dx * dz/dy",
   "id": "179801da6791c9a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:17:07.787743Z",
     "start_time": "2026-02-10T08:17:07.783434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This is the derivative function without the use of pytorch.\n",
    "import math\n",
    "\n",
    "def dz_dx(x):\n",
    "    return 2 * x * math.cos(x**2)\n",
    "\n",
    "dz_dx(4)"
   ],
   "id": "a1baba2ec9641ff8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.661275842587077"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:17:32.063745Z",
     "start_time": "2026-02-10T08:17:32.060803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now, let's use torch lib\n",
    "x = torch.tensor(4.0, requires_grad=True)"
   ],
   "id": "9e541a5c36850717",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:17:52.436509Z",
     "start_time": "2026-02-10T08:17:52.433281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = x ** 2\n",
    "z = torch.sin(y)"
   ],
   "id": "2e0c6ae8c68973b1",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:17:54.208507Z",
     "start_time": "2026-02-10T08:17:54.203408Z"
    }
   },
   "cell_type": "code",
   "source": "x",
   "id": "143accccdf6aea0b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:17:56.326075Z",
     "start_time": "2026-02-10T08:17:56.319948Z"
    }
   },
   "cell_type": "code",
   "source": "y",
   "id": "33ff7fc8c04ffdc3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:17:58.122364Z",
     "start_time": "2026-02-10T08:17:58.117107Z"
    }
   },
   "cell_type": "code",
   "source": "z",
   "id": "d6999cde7066e319",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2879, grad_fn=<SinBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:18:47.448150Z",
     "start_time": "2026-02-10T08:18:47.445413Z"
    }
   },
   "cell_type": "code",
   "source": "z.backward()",
   "id": "68046e6d60d0613d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:18:52.239555Z",
     "start_time": "2026-02-10T08:18:52.234416Z"
    }
   },
   "cell_type": "code",
   "source": "x.grad",
   "id": "250b337d7cd4f6ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-7.6613)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### See, we have same values of x with backward function and without backward function\n",
    "\n",
    "### dz_dx = -7.661275... x.grad = -7.6613"
   ],
   "id": "653d897ac45928c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Now, we will build a basic and simple model with autograd and without autograd feature.",
   "id": "c8d05647377371b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:33:23.505536Z",
     "start_time": "2026-02-10T08:33:23.502378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inputs\n",
    "x = torch.tensor(6.7)  # Input feature\n",
    "y = torch.tensor(0.0)  # True label (binary)\n",
    "\n",
    "# Randomly select values of w and b. It could be any numbers just for fun.\n",
    "w = torch.tensor(1.0)  # Weight\n",
    "b = torch.tensor(0.0)  # Bias"
   ],
   "id": "7ef81cd47c17ddff",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:34:59.942370Z",
     "start_time": "2026-02-10T08:34:59.939298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Forward pass\n",
    "\n",
    "z = w * x + b  # Weighted sum (linear part)\n",
    "y_pred = torch.sigmoid(z)  # Predicted probability"
   ],
   "id": "64806601c870c98f",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:35:48.354760Z",
     "start_time": "2026-02-10T08:35:48.350954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Compute loss using  Binary Cross-Entropy Loss\n",
    "def binary_cross_entropy_loss(prediction, target):\n",
    "    epsilon = 1e-8  # To prevent log(0)\n",
    "    prediction = torch.clamp(prediction, epsilon, 1 - epsilon)\n",
    "    return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction))\n"
   ],
   "id": "a3c9efee00374463",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:35:56.426454Z",
     "start_time": "2026-02-10T08:35:56.420727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute binary cross-entropy loss\n",
    "loss = binary_cross_entropy_loss(y_pred, y)\n",
    "loss"
   ],
   "id": "a32938b5e71c1877",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7012)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:36:54.920229Z",
     "start_time": "2026-02-10T08:36:54.916110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Backward pass: Derivatives\n",
    "\n",
    "# 1. dL/d(y_pred): Loss with respect to the prediction (y_pred)\n",
    "dloss_dy_pred = (y_pred - y)/(y_pred*(1-y_pred))\n",
    "\n",
    "# 2. dy_pred/dz: Prediction (y_pred) with respect to z (sigmoid derivative)\n",
    "dy_pred_dz = y_pred * (1 - y_pred)\n",
    "\n",
    "# 3. dz/dw and dz/db: z with respect to w and b\n",
    "dz_dw = x  # dz/dw = x\n",
    "dz_db = 1  # dz/db = 1 (bias contributes directly to z)\n",
    "\n",
    "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
    "dL_db = dloss_dy_pred * dy_pred_dz * dz_db"
   ],
   "id": "aa501ca7bb8555ca",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:37:01.329045Z",
     "start_time": "2026-02-10T08:37:01.325278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw}\")\n",
    "print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db}\")"
   ],
   "id": "391ea015ded4cc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Gradient of loss w.r.t weight (dw): 6.691762447357178\n",
      "Manual Gradient of loss w.r.t bias (db): 0.998770534992218\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:37:45.735799Z",
     "start_time": "2026-02-10T08:37:45.732589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now, using with auto-grad\n",
    "\n",
    "x = torch.tensor(6.7)\n",
    "y = torch.tensor(0.0)"
   ],
   "id": "5bd082dd6b576f98",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:37:50.507622Z",
     "start_time": "2026-02-10T08:37:50.502886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)"
   ],
   "id": "1c4b8822b88b35c6",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:38:10.967399Z",
     "start_time": "2026-02-10T08:38:10.962458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "z = w*x + b\n",
    "z"
   ],
   "id": "6bfef619ef99d175",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7000, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:38:32.400753Z",
     "start_time": "2026-02-10T08:38:32.396266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred = torch.sigmoid(z)\n",
    "y_pred"
   ],
   "id": "b6ba8e7c071345d9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9988, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:39:06.453625Z",
     "start_time": "2026-02-10T08:39:06.448828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss = binary_cross_entropy_loss(y_pred, y)\n",
    "loss"
   ],
   "id": "415459be826199fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7012, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:39:17.560003Z",
     "start_time": "2026-02-10T08:39:17.556397Z"
    }
   },
   "cell_type": "code",
   "source": "loss.backward()",
   "id": "fbc46eb24ba87035",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:39:22.596816Z",
     "start_time": "2026-02-10T08:39:22.591464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(w.grad)\n",
    "print(b.grad)"
   ],
   "id": "ea44eb78d484e93e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6918)\n",
      "tensor(0.9988)\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Woow! Using backward and grad, we can easily compute derivatives which is really good.\n",
    "# Above all examples were scalar means, 1 value. Now, Assume, we have vector\n",
    "# consist of different numbers.\n",
    "# Let's see how does it work!"
   ],
   "id": "321a803ac125865f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:45:39.675350Z",
     "start_time": "2026-02-10T08:45:39.672320Z"
    }
   },
   "cell_type": "code",
   "source": "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)",
   "id": "a78c4b7d8e5dc815",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:45:41.544492Z",
     "start_time": "2026-02-10T08:45:41.538828Z"
    }
   },
   "cell_type": "code",
   "source": "x",
   "id": "8d7d6ae6dbb8f226",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:45:54.958663Z",
     "start_time": "2026-02-10T08:45:54.953083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = (x**2).mean()\n",
    "y"
   ],
   "id": "ab14d61a937a2f28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6667, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:46:04.503728Z",
     "start_time": "2026-02-10T08:46:04.500853Z"
    }
   },
   "cell_type": "code",
   "source": "y.backward()",
   "id": "f469e9ced4adb875",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T08:46:18.699593Z",
     "start_time": "2026-02-10T08:46:18.694655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Here, we are using the concept of partial derivatives which is mainly use for vectors. Like There are three values (1.0, 2.0, 3.0) in vector.\n",
    "# Let's them with x1, x2, x3. Now, partial derivative w.r.t x1 will dy/dx1 = (2x1) / 3 , dy/dx2 = (2x2) / 3, dy/dx3 = (2x3) / 3. And 3 is coming from mean function (like average).\n",
    "# This is how backward works on vector and show values using grad.\n",
    "x.grad"
   ],
   "id": "1dab143ec1c1d923",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6667, 1.3333, 2.0000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T09:01:39.965959Z",
     "start_time": "2026-02-10T09:01:39.960788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# If you run y.backward multiple times, then value of x like x.grad will difference every time like 4, 16, etc which is not good.\n",
    "# We will use clear_grad to fix the value of x.grad\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "y"
   ],
   "id": "7e983c3a232f3e42",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T09:01:40.234042Z",
     "start_time": "2026-02-10T09:01:40.230772Z"
    }
   },
   "cell_type": "code",
   "source": "y.backward()",
   "id": "4de51e0bf2137a76",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T09:01:40.617651Z",
     "start_time": "2026-02-10T09:01:40.612353Z"
    }
   },
   "cell_type": "code",
   "source": "x.grad",
   "id": "a4756e6fe3ea2328",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T09:03:10.738834Z",
     "start_time": "2026-02-10T09:03:10.733720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x.grad.zero_()\n",
    "# After run this cell, x will 0. So, you'll have to run above three cell again to get x.grad value"
   ],
   "id": "21597ce7c2bb83df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T09:04:36.759489Z",
     "start_time": "2026-02-10T09:04:36.754170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We can disable tracking of variables.\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "y"
   ],
   "id": "4e74fa0732716e61",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T09:04:45.805816Z",
     "start_time": "2026-02-10T09:04:45.802719Z"
    }
   },
   "cell_type": "code",
   "source": "y.backward()",
   "id": "9bf6addbefe9808e",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T09:04:49.482130Z",
     "start_time": "2026-02-10T09:04:49.477767Z"
    }
   },
   "cell_type": "code",
   "source": "x.grad",
   "id": "95fa5228f9a7b505",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T09:05:30.332186Z",
     "start_time": "2026-02-10T09:05:30.293311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    y = x ** 2\n",
    "    y.backward()\n",
    "\n",
    "# Note: It will cause error, because we are disabling the track graph using torch.no_grad"
   ],
   "id": "6415b30cb7adf412",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[55], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m      2\u001B[0m     y \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m----> 3\u001B[0m     \u001B[43my\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pytorch/.venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pytorch/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cb3bb24128b75a15"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
