{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "## Note: In basic-pipeline code file, I code the neural network, sigmoid function, loss function,\n",
    "\n",
    "## update parameters from scratch.\n",
    "\n",
    "## Now, we'll use nn module to speedup and optimize the code.\n",
    "\n",
    "## With the help of nn modules, we can create neural network easily and efficiently. Like torch.sigmoid, torch.optim etc.\n"
   ],
   "id": "51f9c393c3073f08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T07:48:24.482183Z",
     "start_time": "2026-02-11T07:48:24.478979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "id": "f3fb083f90163dea",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T07:48:25.886840Z",
     "start_time": "2026-02-11T07:48:25.881777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assume, we are creating binary classification. In input layer, we have 5 features i.e weights, and using sigmoid activation function and there is one output.\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, num_features):\n",
    "\n",
    "        # super().__init__() is use to access the parent functions. For example: nn.Module is parent, Class model is child. I want to access the all functions even constructors of parent\n",
    "        # function which is nn.Module. When I write this, it also means to register my custom model in nn.Module and with the help of this, I can detect parameters and optimizer will run\n",
    "        # successfully.\n",
    "        # WARNING: If you don't write this, you may not have fully access to parent functions, and can not register your model, can not detect parameters as well, which cause the optimizer\n",
    "        # to fail\n",
    "        super().__init__()\n",
    "\n",
    "        # Let's build neural network with the help of nn modules\n",
    "        # Here, nn.Linear means fully connected layer. For example: if num_feature is 5 i.e 5 features (input numbers) and then output size is 1.\n",
    "        # NOTE: Here, we define the model architecture (layers and neurons).\n",
    "        # No computation happens here — we are only setting up the structure.\n",
    "\n",
    "        self.linear = nn.Linear(num_features, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    # Here, The forward() method defines how data passes through the model.\n",
    "\n",
    "    def forward(self, features):\n",
    "\n",
    "        # Here, features is the full input tensor (rows and columns), e.g., shape (10,5),\n",
    "        # not just the number of features. The linear layer computes output = x*W + b,\n",
    "        # resulting in shape (10,1) after multiplying (10,5) with weights of shape (5,1).\n",
    "        output = self.linear(features)\n",
    "\n",
    "        # Apply the activation function (sigmoid).\n",
    "        # This keeps the same shape (10,1) but converts values to range [0,1].output = self.activation(output)\n",
    "        return output"
   ],
   "id": "881cdcf2d8c45c62",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T08:40:33.767633Z",
     "start_time": "2026-02-11T08:40:33.761993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a dummy dataset with 10 samples and 5 features\n",
    "data = torch.randn([10, 5])\n",
    "\n",
    "# Create a model object.\n",
    "# data.shape[1] gives the number of features (columns) to define input size.\n",
    "model = Model(data.shape[1])\n",
    "\n",
    "# Pass data through the model to get output probabilities.\n",
    "# You can use model(data) instead of model.forward(data),\n",
    "# because PyTorch automatically calls forward().\n",
    "# NOTE: It's best to use model(data), because, In industry, they follow this model(data), not model.feature(data)\n",
    "output = model(data)\n",
    "print(output)\n"
   ],
   "id": "1f875e3031757fc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5347],\n",
      "        [0.4857],\n",
      "        [0.5416],\n",
      "        [0.2007],\n",
      "        [0.2530],\n",
      "        [0.3960],\n",
      "        [0.5203],\n",
      "        [0.2011],\n",
      "        [0.4441],\n",
      "        [0.2977]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T08:41:28.391943Z",
     "start_time": "2026-02-11T08:41:28.386503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# You can see model weights and bias as well.\n",
    "model.linear.weight\n",
    "model.linear.bias"
   ],
   "id": "aad922c7e0051a23",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.3241], requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# If you want to visualize the model parameters, then you can use torchinfo for this.",
   "id": "104eaf1ff5f1d65f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T08:44:36.418412Z",
     "start_time": "2026-02-11T08:44:36.412042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=data.shape)"
   ],
   "id": "a5adf5b099b309f5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Model                                    [10, 1]                   --\n",
       "├─Linear: 1-1                            [10, 1]                   6\n",
       "├─Sigmoid: 1-2                           [10, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 6\n",
       "Trainable params: 6\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Did you notice, above neural network was for 1 neuron.\n",
    "# Now, let's create a bit complex neural network i.e hidden layer with number of neurons"
   ],
   "id": "fe2874802776b94f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T09:13:56.726102Z",
     "start_time": "2026-02-11T09:13:56.721346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assume, we are creating a binary classification model with one hidden layer.\n",
    "# Input layer has 5 features means 5 neurons, hidden layer has 3 neurons,\n",
    "# and output layer has 1 neuron with sigmoid activation function.\n",
    "\n",
    "class complex_Model(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, num_features):\n",
    "\n",
    "        # super().__init__() is used to access the parent class nn.Module.\n",
    "        # This registers your custom model in PyTorch so that parameters can be tracked,\n",
    "        # and optimizer can update them during training.\n",
    "        super().__init__()\n",
    "\n",
    "        # First fully connected (linear) layer: input features -> 3 hidden neurons\n",
    "        # nn.Linear creates weights and bias for this layer.\n",
    "        # NOTE: This only defines the layer architecture, no computation happens here.\n",
    "        self.linear1 = nn.Linear(num_features, 3)\n",
    "\n",
    "        # Activation function for hidden layer\n",
    "        # ReLU introduces non-linearity to the network.\n",
    "        self.ReLU = nn.ReLU()\n",
    "\n",
    "        # Second fully connected (linear) layer: 3 hidden neurons -> 1 output\n",
    "        self.linear2 = nn.Linear(3, 1)\n",
    "\n",
    "        # Activation function for output layer\n",
    "        # Sigmoid converts output to probability (range 0 to 1) for binary classification.\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # Forward pass: defines how input data flows through the network\n",
    "    def forward(self, features):\n",
    "\n",
    "        # Pass input through first linear layer\n",
    "        # features is the full input tensor (rows = samples, columns = features), e.g., shape (10,5)\n",
    "        # Linear layer computes output = x*W + b\n",
    "        # Output shape after this layer: (10,3)\n",
    "        output = self.linear1(features)\n",
    "\n",
    "        # Apply ReLU activation to hidden layer\n",
    "        # Keeps the shape (10,3) but introduces non-linearity\n",
    "        output = self.ReLU(output)\n",
    "\n",
    "        # Pass through second linear layer\n",
    "        # Computes weighted sum of hidden neurons for the output layer\n",
    "        # Output shape: (10,1)\n",
    "        output = self.linear2(output)\n",
    "\n",
    "        # Apply sigmoid activation for binary classification\n",
    "        # Output shape remains (10,1), values between 0 and 1\n",
    "        output = self.sigmoid(output)\n",
    "\n",
    "        return output\n"
   ],
   "id": "8e45593a563640f",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T09:29:57.117650Z",
     "start_time": "2026-02-11T09:29:57.111589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_data = torch.randn([10, 5])\n",
    "model1 = complex_Model(new_data.shape[1])\n",
    "\n",
    "model1(data)"
   ],
   "id": "9ba9327211dc0478",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4867],\n",
       "        [0.4831],\n",
       "        [0.4852],\n",
       "        [0.5157],\n",
       "        [0.5152],\n",
       "        [0.4852],\n",
       "        [0.4800],\n",
       "        [0.5213],\n",
       "        [0.4856],\n",
       "        [0.4908]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T09:30:11.476170Z",
     "start_time": "2026-02-11T09:30:11.469968Z"
    }
   },
   "cell_type": "code",
   "source": "model1.linear1.weight",
   "id": "a58d080de794c185",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4351,  0.2339, -0.1694, -0.1105,  0.3747],\n",
       "        [ 0.3863, -0.0105,  0.3144, -0.0650, -0.1686],\n",
       "        [-0.2292,  0.1392, -0.0284,  0.3103,  0.2661]], requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T09:30:25.197902Z",
     "start_time": "2026-02-11T09:30:25.193146Z"
    }
   },
   "cell_type": "code",
   "source": "model1.linear2.weight",
   "id": "a070576a6e37d7f7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0984, -0.0749,  0.1057]], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T09:30:45.942306Z",
     "start_time": "2026-02-11T09:30:45.937025Z"
    }
   },
   "cell_type": "code",
   "source": "model1.linear2.bias",
   "id": "c15972b6dd0f5065",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0593], requires_grad=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T09:26:51.535348Z",
     "start_time": "2026-02-11T09:26:51.529389Z"
    }
   },
   "cell_type": "code",
   "source": "summary(model1, new_data.shape)",
   "id": "db7b123c19abff2f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "complex_Model                            [10, 1]                   --\n",
       "├─Linear: 1-1                            [10, 3]                   18\n",
       "├─ReLU: 1-2                              [10, 3]                   --\n",
       "├─Linear: 1-3                            [10, 1]                   4\n",
       "├─Sigmoid: 1-4                           [10, 1]                   --\n",
       "==========================================================================================\n",
       "Total params: 22\n",
       "Trainable params: 22\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# If we want to build a neural network with many hidden layers (e.g., 10+)\n",
    "# and each layer has 10–50 neurons, writing each layer manually is hard and repetitive.\n",
    "# To simplify this, we can use nn.Sequential to build the network easily and make it reusable.\n",
    "# The code of simplified version is given below:"
   ],
   "id": "da157ccaad22d33f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T09:39:38.598221Z",
     "start_time": "2026-02-11T09:39:38.594358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assume we are building a binary classification model with one hidden layer.\n",
    "# Input has 'num_features', hidden layer has 3 neurons, and output has 1 neuron with sigmoid activation.\n",
    "\n",
    "class complex_Model(nn.Module):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, num_features):\n",
    "\n",
    "        # Call parent class constructor to register model parameters\n",
    "        super().__init__()\n",
    "\n",
    "        # Using nn.Sequential to define the network in a simple and reusable way\n",
    "        # nn.Sequential allows us to chain layers in the order they are applied\n",
    "        # Here:\n",
    "        # 1. Linear layer: input -> 3 hidden neurons\n",
    "        # 2. ReLU activation\n",
    "        # 3. Linear layer: 3 hidden neurons -> 1 output\n",
    "        # 4. Sigmoid activation to output probability\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(num_features, 3),  # input layer -> hidden layer\n",
    "            nn.ReLU(),                    # activation for hidden layer\n",
    "            nn.Linear(3, 1),              # hidden layer -> output layer\n",
    "            nn.Sigmoid()                  # output activation (binary classification)\n",
    "        )\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, features):\n",
    "        # features is the input tensor (batch_size x num_features), e.g., (10,5)\n",
    "        # Sequential automatically passes input through all layers in order\n",
    "        output = self.network(features)\n",
    "        return output\n"
   ],
   "id": "7e1e84a2c0a813c7",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T09:40:00.865119Z",
     "start_time": "2026-02-11T09:40:00.859011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model2 = complex_Model(new_data.shape[1])\n",
    "model2(data)"
   ],
   "id": "2335583fcb9ce30c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5062],\n",
       "        [0.6025],\n",
       "        [0.7929],\n",
       "        [0.5657],\n",
       "        [0.5628],\n",
       "        [0.6278],\n",
       "        [0.6883],\n",
       "        [0.5118],\n",
       "        [0.7380],\n",
       "        [0.6997]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "644bbbf3be098b27"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
